<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Bracero Program Testimonies Project</title>
    <link rel="stylesheet" href="styles.css"> <!-- Link to an external CSS file -->
</head>
<body>
    <header>
        <h1>Sentiment Analysis Project</h1>
        <nav>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#glove-word2vec">GloVe and Word2Vec</a></li>
                <li><a href="#frequency-tfidf">Frequency Encoding and Term Frequency Inverse Document Frequency (TF-IDF)</a></li>

                <li><a href="#sentiment-testimonials">Sentiment Analysis of Testimonials</a></li>
                <li><a href="network-testimonials ">Network Analysis of Testimonials</a></li>
                
            
            </ul>
        </nav>
    </header>

    <main>
        <section id="overview">
            <h2>Overview</h2>
            <p>This project aims to use language processing tools to analyze testimonies from people affected by the Bracero Program, an agreement between the United States and Mexico that permitted Mexican men to work in agriculture and railroad construction on short-term labor contracts from between the years 1942 and 1964. While the government ensured that the braceros would be protected from unfair wages and discrimination, many laborers faced abuse and unfair treatment and discriminaton from their employers, were exposed to dangerous working conditions, and dealt with unfair employment practices. 

                We downloaded testimonies from the Bracero Oral History project from the University of Texas at El Paso, and the Bracero History Archive. We also downloaded U.S. government documents related to the Bracero Movement. After converting our data to .txt files, we cleaned the data using Regular Expressions (REGEX) and removed the stop words from the files, forming one .txt file for each corpus. The team used multiple different tools to analyze the data, including GloVe, Word2Vec, Frequency Encoding, and Term Frequency Inverse Document Frequency (TF-IDF). 
                </p>
                <p>This project was created by Sophia Calderon Monarrez, Nuria Sanchez Matias, Ji Yeon Ahn, and Grace Aitken as part of the Digital Humanities Fall 2024 course.</p>
        </section>

        <section id="glove-word2vec">
            <h2>GloVe and Word2Vec</h2>
            <p><a href="https://graceaitken7.github.io/DigiHums-Project/gloveword2vec">By Nuria Sanchez with Google Colab</a></p>
                <h3>Rationale</h3>
                 <p> Word embedding analyses give distant reading a semantic and syntactic basis for text interpretation. Beyond dictionary definitions, word embeddings like GloVe (Global Vectors for Word Representation, Stanford University) and Word2Vec (Google) give numerical representations of the similarities between words according to their presence in similar contexts. Based on trained vectors available for downloading, both tools measure the closeness between any given words in an English language corpus, although GloVe is a multilingual model and supported the Spanish corpus in our project as well.</p>  
                
                 <p>I decided to focus my analysis on word embeddings for two reasons. Firstly, because I could measure co-occurrences within our project’s corpora while simultaneously comparing it with other existing corpora, which could showcase to what extent these testimonies are different to the common knowledge about the Bracero Program. Secondly, because it sheds light on the substructures of each testimony, which might not be as easily perceived in a close reading.</p> 
                
                <h3>Methods</h3>
                 <p>GloVe</p>

<p> Selected the word vectors dataset that best matched our corpora: Wikipedia+Gigawork (printed press) 6 billion (400k words) tokens for BHA and Gov.
Ran GloVe in Colab for the word vectors of the 30 most repeated words in each corpora.
Obtained a visualization in Colab of the word vectors to determine which words are relevant either in themselves or in relationship with others for the next step (figures 1, 2 and 3).</p>
<p>Word2Vec</p>

<p> Selected ‘father’, ‘Mexico’, ‘worker’, and ‘money’ for BHA. Ran Word2Vec in Colab each.
Selected ‘government’ and ‘agricultural’ for Gov. Ran Word2Vec in Colab each. Ran ‘Mexico’ and ‘worker’ too to compare with prior. Obtained visualizations. </p>
                
            
                 <h3>Interpretation</h3>
                 <p> GloVe allowed me to pinpoint the main nodes of conversation in each corpus, called clusters in the field of word embeddings. In one hand, BHA orbits around the topic of family (‘father’, ‘mother’, ‘dad’, ‘family’), geography (‘Mexico’, ‘California’, ‘Texas’, ‘camps’), labor (‘work’, ‘years’, ‘time’), and sustenance (‘money’, ‘food’). The topics reappear in the SW corpus, but not arranged in the same way; for instance, ‘time’, ‘work’ and ‘money’ belong to the same category, and ‘field’ is closer to ‘ranch’, ‘town’, and ‘family’, and ‘food’ no longer appears similar to ‘money’. In the other hand, the Gov corpus yielded the topics of political figures (‘ambassador’, ‘president’, ‘embassy’, ‘secretary’), international politics (‘agreement’, ‘relationships’, ‘matter’, ‘United States’, ‘Mexico’, ‘war’), government (‘commision’, ‘government’, ‘department’, ‘state’), and work (‘manpower’, ‘agricultural’, ‘workers’). 
                When ran on its own, that is, without a biased input, Word2Vec yielded interesting constellations that showcased differences, for example, between ‘joy’ and ‘sad’, which is a cluster not relevant enough for the GloVe model; however, after doing research about Word2Vec’s accuracy, I decided to only interpret the analysis I made with the words obtained after the GloVe analysis and interpretation, since it would also give me the opportunity to compare the 3 corpora amongst each other. For example, ‘Mexico’ is similar to ‘work’ in the Gov corpus, whereas it is closer to ‘oranges’, ‘family’ and ‘lives’ in BHA. While this might sound as something evident, other discoveries were found out through the process: when I analysed ‘agricultural’ in Gov, a word adjacent to ‘work’, the word ‘immigration’ appeared, although the program was not conceived as such. As a matter of fact, for the Gov corpus, ‘work’ equals ‘documented’ and ‘solution’, yet upon close reading of the BHA testimonies, many of the bracero relatives mention all the hardships they overcame to either become legal residents or to prove they gained such right. Symptomatically, the BHA second party testimonies consider the workers as ‘able’ people to carry out the job with machines like ‘tractors’, since many former employers and their families mostly remember that from the braceros they hired, but did not socialize further with them. 
                All these paths remain open to further analysis and research, particularly the Spanish language analysis of the SW corpus, and to complement the Word2Vec analysis since it does not measure re-occurence nor frequency.</p>
              
            </p>
        </section>

        <section id="frequency-tfidf">
            <h2>Frequency Encoding and Term Frequency Inverse Document Frequency (TF-IDF)</h2>
            <p> By Sophia Calderón with Google Colab</p>
                <h3>Frequency Encoding</h3>
            <p>Frequency encoding takes the count of words to see which ones occur the most in a given corpus. By using frequency encoding and sorting the .csv files that were returned as output, we first determined that the list of stop words in Spanish was not as developed as the one in English, resulting in the need to manually go over the results to determine the relevancy of the most frequent words.</p>

            <table>
                <tr>
                    <td><img src="images/Frequency%20Encoding%20BHA.png" width="365"></td>
                    <td><img src="images/Frequency%20Encoding%20Gov.png" width="365"></td>
                    <td><img src="images/Frequency%20Encoding%20SW.png" width="365"></td>
                </tr>
            </table>
    
                
            <p>In the BHA archive, “father”, “dad”, and “mother” were among the most frequent words along with “work” and “year”, emphasizing how, in this archive, a lot of children of Braceros were recounting their parents’ experiences. In the Government documents, words like “Mexican”, “workers”, “war”, “ambassador”, “foreign”, “manpower, “work”, “border”, and “contracts” were emphasized, pointing to a more bureaucratic approach to the Braceros program focused on border and work regulations. The frequency of “war” points to how the U.S. looked at the Bracero program as a solution to labor shortages caused by WWII. As for the Oral History transcripts from the ScholarWorks archive, made up of interviews in both English and Spanish, the top 20 words are all in Spanish, pointing to how most Braceros interviewed were more comfortable with Spanish. The emphasis of “trabajo” (work), “trabajar” (working), “familia” (family), and “contracto” (contract), emphasize how Braceros’ main motivations were working to provide for their families.
</p>
                
                <h3>Frequency Inverse Document Frequency (TF-IDF)</h3>
                
                <p>TF-IDF measures how relevant a word is in a corpus. When combined with the Frequency Analysis, this can give valuable insight into which words are the most important as compared to which are the most relevant.</p>

            <table>
                <tr>
                    <td><img src="images/TF-IDF%20BHA.png" width="365"></td>
                    <td><img src="images/TF-IDF%20Gov.png" width="365"></td>
                    <td><img src="images/TF-IDF%20SW.png" width="365"></td>
                </tr>
            </table>
            
            <p>For the BHA archive, “time” is the 3rd most relevant despite not being one of the top 20 in frequency, pointing to how family members may be emphasizing the time family members spent away, or the time they spent working. For the Government documents, “Mexico”, “government”, and “agreement” are ranked higher in the TF-IDF graph than in the Frequency Encoding graph, suggesting the importance of agreements and policies made with the Mexican government. “Return” is ranked 20th in the TF-IDF graph despite not being in the top 20 most frequently used words, pointing to how the government probably emphasized the importance of Mexican workers returning to Mexico after the Braceros program. In the ScholarWorks interview transcript, “tiempo” is the most relevant word, highlighting the time the Braceros spent working, as “trabajo” and “trabajar” are also relevant words. </p>
                
            <section id="sentiment-testimonials">
                <h2>Sentiment Analysis of Testimonials</h2>
                <p> By Ji Yeon Ahn</p>
                
                <p> Pre-Processing Steps </p>
               <p> 1. Gathering the PDF files of testimonials and turning them into TXT files. </p>
              2. Cleaning the TXT files using regular expressions. (Google Colab)
               <p> 3. Translating the Spanish corpus into English using DeepL. This step has a risk of lowering the accuracy of the result, but was inevitable as currently available Python libraries for sentiment analysis weren’t quite capable of processing large amounts of Spanish data (The script either ran for hours or output errors with tags such as “translate” when I ran it with the Spanish parts without translating them into English). </p>
                
               <p> Testing with three different Python NLP libraries (vaderSentiment, TextBlob, nltk) to cross check the result </p>
               
               <td><img src="images/sentiment-analysis-results.png" width="800"></td>
            
            <p>  1. VaderSentiment
              <p>Positive: 0.106 </p>
               <p>This indicates that approximately 10.6% of the text consists of positive emotions or words.</p>
              <p> Neutral: 0.855 </p> 
              <p> 85.5% of the text is neutral, indicating it doesn't express strong emotions, whether positive or negative. It conveys plain information. </p> 
               <p> Negative: 0.039 </p>
              <p> This indicates that 3.9% of the text has negative emotions or words. </p> 
               <p> Overall Compound Score: 1.0 (Positive) </p>
              <p> The compound score ranges from -1.0 to 1.0. Neutral words, which make up 85.5% of the analyzed text, do not influence the compound score either positively or negatively. Although only 10.6% of the words are classified as positive, if these words carry a high intensity it can further contribute to the overall sentiment being calculated as positive. Also, if positive words face little opposition from negative sentiment, it can lead to the overall compound score to lean towards being overall positive. </p> 

               <p> 2. TextBlob
              <p>Polarity: 0.10277644627777315 </p> 
               <p>'Polarity' scores range from -1.0 to 1.0; -1.0 represents very negative sentiment, 0.0 indicates neutral sentiment, and 1.0 signifies very positive sentiment. A score of 0.10277644627777315 suggests that the text has a moderately positive sentiment.</p>
                <p>Subjectivity: 0.417650237939911 </p>
               <p> ‘Subjectivity’ scores range from 0.0 to 1.0, with 0.0 representing an objective recounting of facts rather than feelings, while 1.0 representing complete subjectivity reflecting opinions or feelings. 0.417650237939911 indicates that the text is somewhat balanced but leans slightly towards objectivity. </p>
               <p> The overall sentiment is Positive. </p>
                
                </p>

                <p>3. Nltk
                   <p>Positive: 0.109</p> 
                    <p>10.9% of the text is made up of positive words.</p>
                    <p>Neutral: 0.835</p>
                    <p>83.5% of the text is neutral.</p>
                    <p>Negative: 0.055 </p>
                    <p>5.5% of the text is made up of negative words.</p>
                    <p>Overall Sentiment (Compound): 1.0 (Positive)</p>
                    
                    </p>
               
                <p>Reflections
                    The major limitation of the above sentiment analysis is that it categorizes the text into either positive or negative, thus oversimplifying the complexity of human emotional experience. This binary categorization overlooks the nuances of words, limiting the output to capture a broader range of emotions. However, this seeming limitation can conversely serve as a meaningful warning towards statistical analyses of human experiences. The above result indicating there is more positive sentiment to the testimonials than negative sentiment, is contrary to our existing understanding that the Bracero Program involved mistreatment of the participants. This provokes us to think about how statistical examination of human experience can undermine the severity of existing but not statistically overarching numbers of unfair treatment. In other words, it raises important questions about whether a statistical representation can be considered a “fair” representation.
                    </p>
                
            </section>

            <section id="network-testimonials">
                <h2>Network Analysis of Testimonials</h2>
                <p> By Ji Yeon Ahn</p>
                
                <p>Graphic Visualization of Words Connected to Bracero
                </p>
                <p>Next, I wanted to explore how words relate to the keyword "bracero" and visualize a network displaying these connections. Key tools used in this process were libraries NetworkX and Matplotlib. My window size was 5, defining connection to “bracero” as appearing within 5 words of distance. Although the script incorporated Nltk library’s function of removing stopwords, words such as “y,”  “que,” “este,” etc. still remained, which is the limitation of current NLP libraries. The result below puts the keyword “bracero” in the center, and the more frequent a word appears in the context of “bracero” the closer it is located toward the center of the graph. The numbers on the lines indicate the number of collocations, the number being higher meaning more frequent collocation.</p>

                <td><img src="images/bracero-network-analysis" width="800"></td>

            <p>Network of Collocates
            </p>

            <p>Then, I did an additional analysis examining the relationships between the collocates to uncover central themes in the text. The script first identifies the 50 most frequent collocates of "bracero," and then it checks if any of these 50 words are closely put together (my window size was 5 words before and 5 words after). If, as an example, "trabajar" and "bien" show up together so-and-so times in different parts of the text, it's recorded as a connection between these two words with a strength of so-and-so number. The strength, or in other words, “weight” is indicated as numbers on the edges/connecting lines of the graph visualization. If a word appears near the center of the graph, it generally indicates that the word is closely connected to many other words in the 50-collocates list. An interesting observation from the results is that positive words such as “bien” and “bueno” are located towards the center of the network. Since these words are connected to many other words, there is a high probability that this word represents a key theme or bridging concept in the text. “Bien” and “Bueno” being located towards the center aligns with the sentiment analysis results that analyzed the corpus as having an overall positive sentiment. 
            </p>

            <td><img src="images/network-of-collocates-of-bracero" width="800"></td>



    </main>
    </footer>
</body>
</html>
